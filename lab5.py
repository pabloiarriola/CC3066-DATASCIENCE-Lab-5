# -*- coding: utf-8 -*-
"""Lab5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FAvalsk0J0_z6uDH7QxNrLR5T4ogDDVx

# Laboratorio 5

---
Se necesita tener acceso los archivos de google drive para poder usar este código, aqui esta el link de compartir: <br>[Google Drive](https://drive.google.com/open?id=1qHCI8CdRZRUQJ9cHdwLO0OT4czG5c7r2) <br>

--- 
Tabla de contenido


*   Análisis Exploratorio

#Importaciones
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from google.colab import files
import matplotlib.pyplot as plt
import re
import io 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
from nltk import ngrams
import heapq
import operator
# Load library
from nltk.corpus import stopwords
import os
# You will have to download the set of stop words the first time
import nltk
nltk.download('stopwords')

"""### Conexión a Google Drive"""

!pip install -U -q PyDrive ## you will have install for every colab session
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file_list = drive.ListFile({'q': "'1qHCI8CdRZRUQJ9cHdwLO0OT4czG5c7r2' in parents and trashed=false"}).GetList()
for file1 in file_list:
  print('title: %s, id: %s' % (file1['title'], file1['id']))

"""#Descargar el archivo de drive"""

GrammarandProductReviews = drive.CreateFile({'id': '1fGkPkrk96zg14OBcsGjFkOV8yJJpuJ_n'})
GrammarandProductReviews.GetContentFile('GrammarandProductReviews.csv')

dataset = pd.read_csv('GrammarandProductReviews.csv')
# dataset=pd.read_csv('GrammarandProductReviews.csv',sep='\t')

"""###Limpieza de archivos"""

dataset = dataset.rename(columns={'reviews.text':'reviewtext'})

dataset.reviewtext

dataset.reviewtext = dataset.reviewtext.str.lower()
dataset.reviewtext.replace(regex=r'-', value=' ')
dataset.reviewtext.replace(regex=r'@', value=' ')
dataset.reviewtext.replace(regex=r'#', value=' ')
dataset.reviewtext.replace(regex=r'$', value=' ')
dataset.reviewtext.replace(regex=r'%', value=' ')
dataset.reviewtext.replace(regex=r'&', value=' ')
dataset.reviewtext.replace(regex=r'\.', value=' ')
dataset.reviewtext.replace(regex=r'\'', value='')
dataset.reviewtext.replace(regex=r'!', value=' ')

dataset

lowerYsub('GrammarandProductReviews.csv','GrammarandProductReviews2.csv')

"""###Quitar Stopwords"""

import io 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')
stop_words = set(stopwords.words('english')) 


def StopWordRemover2000(filename, filewrite):
  file1 = open(filename ,encoding="UTF-8") 
  line = file1.read()# Use this to read file content as a stream: 
  words = line.split() 
  for r in words: 
      if not r in stop_words: 
          appendFile = open(filewrite,'a',encoding="UTF-8") 
          appendFile.write(" "+r) 
          appendFile.close()
  print('Success')

StopWordRemover2000('GrammarandProductReviews3.csv', 'GrammarandProductReviews4.csv')

os.remove('GrammarandProductReviews.csv')
os.remove('GrammarandProductReviews2.csv')
os.remove('GrammarandProductReviews3.csv')
os.rename('GrammarandProductReviews4.csv','GrammarandProductReviews.csv')

"""### Clasificacion de Palabra"""

